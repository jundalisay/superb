IV. Rouge et Noir.‚ÄîThe questions raised by games of
chance, such as roulette, are, fundamentally, quite analo-
gous to those we have just treated. For example, a wheel
is divided into a large number of equal compartments, al-
ternately red and black. A ball is spun round the wheel,
and after having moved round a number of times, it stops
in front of one of these sub-divisions. The probability
that the division is red is obviously 12 . The needle de-
scribes an angle Œ∏, including several complete revolutions.
I do not know what is the probability that the ball is
spun with such a force that this angle should lie between
Œ∏ and Œ∏ + dŒ∏, but I can make a convention. I can suppose
that this probability is œÜ(Œ∏) dŒ∏. As for the function œÜ(Œ∏),
I can choose it in an entirely arbitrary manner. I havethe calculus of probabilities.
225
nothing to guide me in my choice, but I am naturally in-
duced to suppose the function to be continuous. Let  be
a length (measured on the circumference of the circle of
radius unity) of each red and black compartment. We
have to calculate the integral of œÜ(Œ∏) dŒ∏, extending it on
the one hand to all the red, and on the other hand to
all the black compartments, and to compare the results.
Consider an interval 2 comprising two consecutive red
and black compartments. Let M and m be the maxi-
mum and minimum values of the function œÜ(Œ∏) in this
interval. The integral P extended to the red compartments
will be smaller than
M; extended to the black it will
P
be greater than
m.
The difference will therefore be
P
smaller than (M ‚àí m). But if the function œÜ is sup-
posed continuous, and if on the other hand the interval 
is very small with respect to the total angle described by
the needle, the difference M ‚àí m will be very small. The
difference of the two integrals will be therefore very small,
and the probability will be very nearly 2 1 . We see that
without knowing anything of the function œÜ we must act
as if the probability were 12 . And on the other hand it ex-
plains why, from the objective point of view, if I watch a
certain number of coups, observation will give me almost
as many black coups as red. All the players know thisscience and hypothesis
226
objective law; but it leads them into a remarkable error,
which has often been exposed, but into which they are
always falling. When the red has won, for example, six
times running, they bet on black, thinking that they are
playing an absolutely safe game, because they say it is a
very rare thing for the red to win seven times running. In
reality their probability of winning is still 12 . Observation
shows, it is true, that the series of seven consecutive reds
is very rare, but series of six reds followed by a black are
also very rare. They have noticed the rarity of the series
of seven reds; if they have not remarked the rarity of six
reds and a black, it is only because such series strike the
attention less.
V. The Probability of Causes.‚ÄîWe now come to the
problems of the probability of causes, the most impor-
tant from the point of view of scientific applications. Two
stars, for instance, are very close together on the celes-
tial sphere. Is this apparent contiguity a mere effect of
chance? Are these stars, although almost on the same
visual ray, situated at very different distances from the
earth, and therefore very far indeed from one another? or
does the apparent correspond to a real contiguity? This
is a problem on the probability of causes.
First of all, I recall that at the outset of all problemsthe calculus of probabilities.
227
of probability of effects that have occupied our attention
up to now, we have had to use a convention which was
more or less justified; and if in most cases the result was
to a certain extent independent of this convention, it was
only the condition of certain hypotheses which enabled
us √† priori to reject discontinuous functions, for example,
or certain absurd conventions. We shall again find some-
thing analogous to this when we deal with the probability
of causes. An effect may be produced by the cause a or
by the cause b. The effect has just been observed. We
ask the probability that it is due to the cause a. This
is an √† posteriori probability of cause. But I could not
calculate it, if a convention more or less justified did not
tell me in advance what is the √† priori probability for
the cause a to come into play‚ÄîI mean the probability of
this event to some one who had not observed the effect.
To make my meaning clearer, I go back to the game of
√©cart√© mentioned before. My adversary deals for the first
time and turns up a king. What is the probability that
he is a sharper? The formul√¶ ordinarily taught give 9 8 ,
a result which is obviously rather surprising. If we look
at it closer, we see that the conclusion is arrived at as if,
before sitting down at the table, I had considered that
there was one chance in two that my adversary was notscience and hypothesis
228
honest. An absurd hypothesis, because in that case I
should certainly not have played with him; and this ex-
plains the absurdity of the conclusion. The function on
the √† priori probability was unjustified, and that is why
the conclusion of the √† posteriori probability led me into
an inadmissible result. The importance of this prelimi-
nary convention is obvious. I shall even add that if none
were made, the problem of the √† posteriori probability
would have no meaning. It must be always made either
explicitly or tacitly.
Let us pass on to an example of a more scientific
character. I require to determine an experimental law;
this law, when discovered, can be represented by a curve.
I make a certain number of isolated observations, each
of which may be represented by a point. When I have
obtained these different points, I draw a curve between
them as carefully as possible, giving my curve a regular
form, avoiding sharp angles, accentuated inflexions, and
any sudden variation of the radius of curvature. This
curve will represent to me the probable law, and not only
will it give me the values of the functions intermediary to
those which have been observed, but it also gives me the
observed values more accurately than direct observation
does; that is why I make the curve pass near the pointsthe calculus of probabilities.
229
and not through the points themselves.
Here, then, is a problem in the probability of causes.
The effects are the measurements I have recorded; they
depend on the combination of two causes‚Äîthe true law
of the phenomenon and errors of observation. Know-
ing the effects, we have to find the probability that the
phenomenon shall obey this law or that, and that the
observations have been accompanied by this or that er-
ror. The most probable law, therefore, corresponds to the
curve we have traced, and the most probable error is rep-
resented by the distance of the corresponding point from
that curve. But the problem has no meaning if before
the observations I had an √† priori idea of the probability
of this law or that, or of the chances of error to which
I am exposed. If my instruments are good (and I knew
whether this is so or not before beginning the observa-
tions), I shall not draw the curve far from the points
which represent the rough measurements. If they are in-
ferior, I may draw it a little farther from the points, so
that I may get a less sinuous curve; much will be sacri-
ficed to regularity.
Why, then, do I draw a curve without sinuosities?
Because I consider √† priori a law represented by a con-
tinuous function (or function the derivatives of which toscience and hypothesis
230
a high order are small), as more probable than a law not
satisfying those conditions. But for this conviction the
problem would have no meaning; interpolation would be
impossible; no law could be deduced from a finite number
of observations; science would cease to exist.
Fifty years ago physicists considered, other things be-
ing equal, a simple law as more probable than a compli-
cated law. This principle was even invoked in favour
of Mariotte‚Äôs law as against that of Regnault. But this
belief is now repudiated; and yet, how many times are
we compelled to act as though we still held it! However
that may be, what remains of this tendency is the be-
lief in continuity, and as we have just seen, if the belief in
continuity were to disappear, experimental science would
become impossible.
VI. The Theory of Errors.‚ÄîWe are thus brought to
consider the theory of errors which is directly connected
with the problem of the probability of causes. Here again
we find effects‚Äîto wit, a certain number of irreconcilable
observations, and we try to find the causes which are, on
the one hand, the true value of the quantity to be mea-
sured, and, on the other, the error made in each isolated
observation. We must calculate the probable √† posteri-
ori value of each error, and therefore the probable valuethe calculus of probabilities.
231
of the quantity to be measured. But, as I have just ex-
plained, we cannot undertake this calculation unless we
admit √† priori ‚Äîi.e., before any observations are made‚Äî
that there is a law of the probability of errors. Is there
a law of errors? The law to which all calculators assent
is Gauss‚Äôs law, that is represented by a certain transcen-
dental curve known as the ‚Äúbell.‚Äù
But it is first of all necessary to recall the classic dis-
tinction between systematic and accidental errors. If the
metre with which we measure a length is too long, the
number we get will be too small, and it will be no use to
measure several times‚Äîthat is a systematic error. If we
measure with an accurate metre, we may make a mistake,
and find the length sometimes too large and sometimes
too small, and when we take the mean of a large num-
ber of measurements, the error will tend to grow small.
These are accidental errors.
It is clear that systematic errors do not satisfy Gauss‚Äôs
law, but do accidental errors satisfy it? Numerous proofs
have been attempted, almost all of them crude paralo-
gisms. But starting from the following hypotheses we
may prove Gauss‚Äôs law: the error is the result of a very
large number of partial and independent errors; each par-
tial error is very small and obeys any law of probabilityscience and hypothesis
232
whatever, provided the probability of a positive error is
the same as that of an equal negative error. It is clear
that these conditions will be often, but not always, ful-
filled, and we may reserve the name of accidental for er-
rors which satisfy them.
We see that the method of least squares is not legit-
imate in every case; in general, physicists are more dis-
trustful of it than astronomers. This is no doubt because
the latter, apart from the systematic errors to which
they and the physicists are subject alike, have to contend
with an extremely important source of error which is en-
tirely accidental‚ÄîI mean atmospheric undulations. So it
is very curious to hear a discussion between a physicist
and an astronomer about a method of observation. The
physicist, persuaded that one good measurement is worth
more than many bad ones, is pre-eminently concerned
with the elimination by means of every precaution of the
final systematic errors; the astronomer retorts: ‚ÄúBut you
can only observe a small number of stars, and accidental
errors will not disappear.‚Äù
What conclusion must we draw? Must we continue to
use the method of least squares? We must distinguish.
We have eliminated all the systematic errors of which
we have any suspicion; we are quite certain that therethe calculus of probabilities.

are others still, but we cannot detect them; and yet we
must make up our minds and adopt a definitive value
which will be regarded as the probable value; and for
that purpose it is clear that the best thing we can do is
to apply Gauss‚Äôs law. We have only applied a practical
rule referring to subjective probability. And there is no
more to be said.

Yet we want to go farther and say that not only the
probable value is so much, but that the probable error in
the result is so much. This is absolutely invalid : it would
be true only if we were sure that all the systematic errors
were eliminated, and of that we know absolutely nothing.
We have two series of observations; by applying the law
of least squares we find that the probable error in the
first series is twice as small as in the second. The second
series may, however, be more accurate than the first, be-
cause the first is perhaps affected by a large systematic
error. All that we can say is, that the first series is prob-
ably better than the second because its accidental error
is smaller, and that we have no reason for affirming that
the systematic error is greater for one of the series than
for the other, our ignorance on this point being absolute.


VII. Conclusions


In the preceding lines I have set several problems, and have given no solution. I do not re-science and hypothesis gret this, for perhaps they will invite the reader to reflect
on these delicate questions.
However that may be, there are certain points which
seem to be well established. To undertake the calcula-
tion of any probability, and even for that calculation to
have any meaning at all, we must admit, as a point of
departure, an hypothesis or convention which has always
something arbitrary about it. In the choice of this con-
vention we can be guided only by the principle of suffi-
cient reason. Unfortunately, this principle is very vague
and very elastic, and in the cursory examination we have
just made we have seen it assume different forms. The
form under which we meet it most often is the belief in
continuity, a belief which it would be difficult to justify
by apodeictic reasoning, but without which all science
would be impossible. Finally, the problems to which the
calculus of probabilities may be applied with profit are
those in which the result is independent of the hypothe-
sis made at the outset, provided only that this hypothesis
satisfies the condition of continuity.
